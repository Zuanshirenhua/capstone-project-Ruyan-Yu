{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0d4d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               username                                       comment_body  \\\n",
      "0  Embarrassed-Local-79  Thank you so much for taking the time and ener...   \n",
      "1  Embarrassed-Local-79  If you know deep down something was wrong, the...   \n",
      "2  Embarrassed-Local-79  Thank you for the guidance! It worked, I manag...   \n",
      "3  Embarrassed-Local-79  There was definitely some dirt but managed to ...   \n",
      "4  Embarrassed-Local-79  Do you mean more freestyle and improvisation a...   \n",
      "\n",
      "   score         created_utc  \n",
      "0      2 2025-06-09 23:47:18  \n",
      "1      2 2025-06-09 00:45:41  \n",
      "2      2 2025-05-09 15:39:17  \n",
      "3      1 2025-05-09 15:27:57  \n",
      "4      1 2025-05-03 21:52:26  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with your actual Excel file name\n",
    "file_path = \"recover_user_comments.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "ed_comments = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(ed_comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df61ba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       username                                       comment_body  score  \\\n",
      "0  SolidLow9296  Companies don’t care about the welfare of comm...      1   \n",
      "1  SolidLow9296  Aw man, the only position open for Canada has ...      1   \n",
      "2  SolidLow9296  I mean yes I got a pet gate to split the house...      1   \n",
      "3  SolidLow9296  Would you say I’m being too pessimistic? I def...      1   \n",
      "4  SolidLow9296  What are some of your interests? Including it ...      1   \n",
      "\n",
      "          created_utc  \n",
      "0 2025-04-26 16:06:57  \n",
      "1 2025-04-26 03:58:44  \n",
      "2 2024-10-28 02:07:24  \n",
      "3 2024-10-28 01:18:47  \n",
      "4 2024-08-17 12:27:19  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with your actual Excel file name\n",
    "file_path = \"user_comments.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "recover_comments = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(recover_comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6cdc06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               username                                       comment_body  \\\n",
      "0  Embarrassed-Local-79  Thank you so much for taking the time and ener...   \n",
      "1  Embarrassed-Local-79  If you know deep down something was wrong, the...   \n",
      "2  Embarrassed-Local-79  Thank you for the guidance! It worked, I manag...   \n",
      "3  Embarrassed-Local-79  There was definitely some dirt but managed to ...   \n",
      "4  Embarrassed-Local-79  Do you mean more freestyle and improvisation a...   \n",
      "\n",
      "   score         created_utc  \n",
      "0      2 2025-06-09 23:47:18  \n",
      "1      2 2025-06-09 00:45:41  \n",
      "2      2 2025-05-09 15:39:17  \n",
      "3      1 2025-05-09 15:27:57  \n",
      "4      1 2025-05-03 21:52:26  \n"
     ]
    }
   ],
   "source": [
    "ed_comments_cleaned = ed_comments.dropna(subset=['comment_body'])\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(ed_comments_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d24bc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       username                                       comment_body  score  \\\n",
      "0  SolidLow9296  Companies don’t care about the welfare of comm...      1   \n",
      "1  SolidLow9296  Aw man, the only position open for Canada has ...      1   \n",
      "2  SolidLow9296  I mean yes I got a pet gate to split the house...      1   \n",
      "3  SolidLow9296  Would you say I’m being too pessimistic? I def...      1   \n",
      "4  SolidLow9296  What are some of your interests? Including it ...      1   \n",
      "\n",
      "          created_utc  \n",
      "0 2025-04-26 16:06:57  \n",
      "1 2025-04-26 03:58:44  \n",
      "2 2024-10-28 02:07:24  \n",
      "3 2024-10-28 01:18:47  \n",
      "4 2024-08-17 12:27:19  \n"
     ]
    }
   ],
   "source": [
    "recover_comments_cleaned = recover_comments.dropna(subset=['comment_body'])\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(recover_comments_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c5c582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               username                                       comment_body\n",
      "0  Embarrassed-Local-79  Thank you so much for taking the time and ener...\n",
      "1  Embarrassed-Local-79  If you know deep down something was wrong, the...\n",
      "2  Embarrassed-Local-79  Thank you for the guidance! It worked, I manag...\n",
      "3  Embarrassed-Local-79  There was definitely some dirt but managed to ...\n",
      "4  Embarrassed-Local-79  Do you mean more freestyle and improvisation a...\n"
     ]
    }
   ],
   "source": [
    "ed_comments_only = ed_comments_cleaned[['username', 'comment_body']]\n",
    "print(ed_comments_only.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "035251d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       username                                       comment_body\n",
      "0  SolidLow9296  Companies don’t care about the welfare of comm...\n",
      "1  SolidLow9296  Aw man, the only position open for Canada has ...\n",
      "2  SolidLow9296  I mean yes I got a pet gate to split the house...\n",
      "3  SolidLow9296  Would you say I’m being too pessimistic? I def...\n",
      "4  SolidLow9296  What are some of your interests? Including it ...\n"
     ]
    }
   ],
   "source": [
    "recover_comments_only = recover_comments_cleaned[['username', 'comment_body']]\n",
    "print(recover_comments_only.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "513141a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       username                                       title  \\\n",
      "0  SolidLow9296  Missing Graduation Because of Self-Loathe?   \n",
      "1  SolidLow9296               How do I get started (Canada)   \n",
      "2  SolidLow9296                        CPA (Canada) Inquiry   \n",
      "3  SolidLow9296                         Emailing Recruiter    \n",
      "4  SolidLow9296                              Furniture move   \n",
      "\n",
      "                                                body  score  \\\n",
      "0  Hey everyone,\\n\\nI graduate on Thursday (bache...      4   \n",
      "1  Does anyone have any advice on how to actually...      6   \n",
      "2  Is the only way to become CPA licensed by work...      0   \n",
      "3  Is it generally discouraged to email a recruit...      2   \n",
      "4  I wanna move some furniture including my L sha...      0   \n",
      "\n",
      "                                                 url  created_utc  \\\n",
      "0  https://reddit.com/r/EatingDisorders/comments/...   1749352117   \n",
      "1  https://reddit.com/r/AMLCompliance/comments/1k...   1745637667   \n",
      "2  https://reddit.com/r/Accounting/comments/1f7ba...   1725295140   \n",
      "3  https://reddit.com/r/KPMG/comments/1d5v9vy/ema...   1717271551   \n",
      "4  https://reddit.com/r/londonontario/comments/18...   1702927487   \n",
      "\n",
      "          created_time  \n",
      "0  2025-06-08 03:08:37  \n",
      "1  2025-04-26 03:21:07  \n",
      "2  2024-09-02 16:39:00  \n",
      "3  2024-06-01 19:52:31  \n",
      "4  2023-12-18 19:24:47  \n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual Excel file name\n",
    "file_path = \"user_posts.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "ed_posts = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(ed_posts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee7e78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               username                                              title  \\\n",
      "0  Embarrassed-Local-79  I'm afraid my sister's ED is causing me to rel...   \n",
      "1  Embarrassed-Local-79  (TW ED) I fear relapsing because of my sister'...   \n",
      "2  Embarrassed-Local-79        Canon EOS 7D: help needed to clean the lens   \n",
      "3  Embarrassed-Local-79           What can I do to become a better dancer?   \n",
      "4  Embarrassed-Local-79  Copenhagen weekend: how much should I budget f...   \n",
      "\n",
      "                                                body  score  \\\n",
      "0  \\nMy sister (16F) has been recently diagnosed ...      6   \n",
      "1  My sister (16F) has been recently diagnosed as...      1   \n",
      "2  Hi guys!\\n\\nWarning: I do not know my way arou...      4   \n",
      "3  Hi! I want to know if any skilled or experienc...      3   \n",
      "4  Hi guys!! I'm planning a weekend with a friend...      3   \n",
      "\n",
      "                                                 url  created_utc  \n",
      "0  https://reddit.com/r/EatingDisorders/comments/...   1749429745  \n",
      "1  https://reddit.com/r/offmychest/comments/1l6mm...   1749416055  \n",
      "2  https://reddit.com/r/canon/comments/1kiju98/ca...   1746800868  \n",
      "3  https://reddit.com/r/Dance/comments/1k9gf88/wh...   1745793120  \n",
      "4  https://reddit.com/r/Europetravel/comments/1i3...   1737102845  \n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual Excel file name\n",
    "file_path = \"recover_user_posts.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "recover_posts = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(recover_posts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caf02721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       username                                               body\n",
      "0  SolidLow9296  Hey everyone,\\n\\nI graduate on Thursday (bache...\n",
      "1  SolidLow9296  Does anyone have any advice on how to actually...\n",
      "2  SolidLow9296  Is the only way to become CPA licensed by work...\n",
      "3  SolidLow9296  Is it generally discouraged to email a recruit...\n",
      "4  SolidLow9296  I wanna move some furniture including my L sha...\n"
     ]
    }
   ],
   "source": [
    "ed_posts_cleaned = ed_posts.dropna(subset=['body'])\n",
    "ed_posts_only = ed_posts_cleaned[['username', 'body']]\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(ed_posts_only.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12cf0931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               username                                               body\n",
      "0  Embarrassed-Local-79  \\nMy sister (16F) has been recently diagnosed ...\n",
      "1  Embarrassed-Local-79  My sister (16F) has been recently diagnosed as...\n",
      "2  Embarrassed-Local-79  Hi guys!\\n\\nWarning: I do not know my way arou...\n",
      "3  Embarrassed-Local-79  Hi! I want to know if any skilled or experienc...\n",
      "4  Embarrassed-Local-79  Hi guys!! I'm planning a weekend with a friend...\n"
     ]
    }
   ],
   "source": [
    "recover_posts_cleaned = recover_posts.dropna(subset=['body'])\n",
    "recover_posts_only = recover_posts_cleaned[['username', 'body']]\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(recover_posts_only.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "757ca75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             username                                       comment_body  \\\n",
      "0  --TheKingOfCards--                                                NaN   \n",
      "1        -maquixtia2-  Finally moved in with my boyfriend after 4 yea...   \n",
      "2               -maru  Caloric beverages, maybe, if you're finding ea...   \n",
      "3      0nceUponATime0  everyone is different but honestly probably no...   \n",
      "4         123RoSeY321                                                NaN   \n",
      "\n",
      "                                                body  \\\n",
      "0  Had my schedule for the day screwed up, which ...   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4  Today I have been thinking about food and my b...   \n",
      "\n",
      "                                           full_text  \n",
      "0   Had my schedule for the day screwed up, which...  \n",
      "1  Finally moved in with my boyfriend after 4 yea...  \n",
      "2  Caloric beverages, maybe, if you're finding ea...  \n",
      "3  everyone is different but honestly probably no...  \n",
      "4   Today I have been thinking about food and my ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Group and concatenate comments by user\n",
    "ed_comments_grouped = ed_comments.groupby('username')['comment_body'].apply(lambda x: ' '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Step 2: Group and concatenate posts by user\n",
    "ed_posts_grouped = ed_posts_only.groupby('username')['body'].apply(lambda x: ' '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Step 3: Merge both by username\n",
    "ed_combined = pd.merge(ed_comments_grouped, ed_posts_grouped, on='username', how='outer')\n",
    "\n",
    "# Step 4: Optionally combine comment + post text into a single field\n",
    "ed_combined['full_text'] = ed_combined[['comment_body', 'body']].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "print(ed_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cfa9630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             username                                       comment_body  \\\n",
      "0  --TheKingOfCards--  ![gif](giphy|muNcDSuINVr1e)\\n\\nbig boss? can y...   \n",
      "1        -maquixtia2-                                                NaN   \n",
      "2               -maru                                                NaN   \n",
      "3      0nceUponATime0                                                NaN   \n",
      "4             1-kit-1                                                NaN   \n",
      "\n",
      "                                                body  \\\n",
      "0                                                NaN   \n",
      "1  I really really want to binge but I’m two mont...   \n",
      "2  I’ve only heard good things about the S1 on th...   \n",
      "3  [removed] [removed] im about six months into r...   \n",
      "4  I am a trans guy and am having some trouble fi...   \n",
      "\n",
      "                                           full_text  \n",
      "0  ![gif](giphy|muNcDSuINVr1e)\\n\\nbig boss? can y...  \n",
      "1   I really really want to binge but I’m two mon...  \n",
      "2   I’ve only heard good things about the S1 on t...  \n",
      "3   [removed] [removed] im about six months into ...  \n",
      "4   I am a trans guy and am having some trouble f...  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Group and concatenate comments by user\n",
    "recover_comments_grouped = recover_comments.groupby('username')['comment_body'].apply(lambda x: ' '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Step 2: Group and concatenate posts by user\n",
    "recover_posts_grouped = recover_posts_only.groupby('username')['body'].apply(lambda x: ' '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Step 3: Merge both by username\n",
    "recover_combined = pd.merge(recover_comments_grouped, recover_posts_grouped, on='username', how='outer')\n",
    "\n",
    "# Step 4: Optionally combine comment + post text into a single field\n",
    "recover_combined['full_text'] = recover_combined[['comment_body', 'body']].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "print(recover_combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44ffe7",
   "metadata": {},
   "source": [
    "### internet slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4e676da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Term            Definition\n",
      "0      $                 money\n",
      "1    $_$             has money\n",
      "2     %)                 drunk\n",
      "3      &                   and\n",
      "4  &apos  wrongly displayed 's\n"
     ]
    }
   ],
   "source": [
    "slang_modified_df = pd.read_csv(\"internet_slang_modified.csv\")\n",
    "\n",
    "# Make all slang terms lowercase\n",
    "slang_modified_df['Term'] = slang_modified_df['Term'].str.lower()\n",
    "slang_modified_df['Definition'] = slang_modified_df['Definition'].str.lower()\n",
    "\n",
    "print(slang_modified_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68eb5bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             username                                 full_text_expanded\n",
      "0  --TheKingOfCards--  had miss you schedule for the day screwed up w...\n",
      "1        -maquixtia2-  finally moved in with miss you boyfriend after...\n",
      "2               -maru  caloric beverages maybe if you re finding eati...\n",
      "3      0nceUponATime0  everyone is different but honestly probably no...\n",
      "4         123RoSeY321  today i have been thinking about marijuana and...\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "# Convert the slang dictionary into a Python dictionary\n",
    "slang_dict = dict(zip(slang_modified_df['Term'].str.lower(), slang_modified_df['Definition']))\n",
    "\n",
    "def replace_slang(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+|\\S+\\.(gif|jpg|jpeg|png|mp4|mov)', '', text)\n",
    "\n",
    "    # Convert emojis to text (e.g., 🍷 -> \":wine_glass:\")\n",
    "    text = emoji.demojize(text, language='en')\n",
    "\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace slang terms word by word\n",
    "    words = text.split()\n",
    "    expanded = [slang_dict.get(word, word) for word in words]\n",
    "    text = ' '.join(expanded)\n",
    "\n",
    "    # Remove all punctuation including unicode\n",
    "    text = ''.join(\n",
    "        ch if not unicodedata.category(ch).startswith('P') else ' '\n",
    "        for ch in text\n",
    "    )\n",
    "\n",
    "    # Remove punctuation using regex (replace all punctuation with space)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "\n",
    "    # Normalize multiple spaces to one\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply the replacement to the 'bio' column\n",
    "ed_combined['full_text_expanded'] = ed_combined['full_text'].apply(replace_slang)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(ed_combined[['username', 'full_text_expanded']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6db67239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             username                                 full_text_expanded\n",
      "0  --TheKingOfCards--  great really good boss can you show united sta...\n",
      "1        -maquixtia2-  i really really want time out binge but i m tw...\n",
      "2               -maru  i ve only heard good things about the s1 drunk...\n",
      "3      0nceUponATime0  removed removed im about six months into recov...\n",
      "4             1-kit-1  i ante meridiem before midday amplitude modula...\n"
     ]
    }
   ],
   "source": [
    "# Apply the replacement to the 'bio' column\n",
    "recover_combined['full_text_expanded'] = recover_combined['full_text'].apply(replace_slang)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(recover_combined[['username', 'full_text_expanded']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e08f8",
   "metadata": {},
   "source": [
    "### stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4f5737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               username                                    comment_cleaned\n",
      "0  Embarrassed-Local-79  thank significant othershout much taking time ...\n",
      "1  Embarrassed-Local-79  know deep something wait see wrong something w...\n",
      "2  Embarrassed-Local-79  thank guidance information technology worked m...\n",
      "3  Embarrassed-Local-79  wait see definitely low grade marijuana gossip...\n",
      "4  Embarrassed-Local-79                  cool freestyle improvisation home\n"
     ]
    }
   ],
   "source": [
    "reddit_bio_stopwords = [\n",
    "    # Basic English stopwords\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n",
    "    'yours', 'yourself', 'yourselves', 'it', 'its', 'itself', 'what', 'which', 'who', 'whom', 'this', 'that',\n",
    "    'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
    "    'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
    "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',\n",
    "    'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then',\n",
    "    'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both',\n",
    "    'each', 'few', 'more', 'most', 'other', 'some', 'such', \n",
    "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just',\n",
    "    'don', 'should', 'now', 'just', 'like', 'also', 'as', 'then', 'now', 'hi', 'hello',\n",
    "\n",
    "    # Reddit/internet specific and informal terms\n",
    "    'm', 'ive', 'id', 'ill', 'youve', 'lets',\n",
    "    'maybe', 'probably', 'honestly', 'literally', 'actually', 'etc',\n",
    "    \n",
    "    # Fillers / vague terms\n",
    "    'stuff', 'things', 'thing', 'lot', 'kinda', 'sorta', 'like', 'okay', 'ok',\n",
    "    'nah', 'yep', 'yeah', 'yay', 'nope', 'hmm', 'uh', 'huh', 'hey', 'hi', 'hello',\n",
    "    'bye', 'pls', 'please', 'thx', 'thanks', 'welcome', 'haha', 'lol', 'lmao','hii', 'heyy', 'heyya', 'yo', 'sup', 'bruh', 'bro'\n",
    "]\n",
    "\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "\n",
    "    # Join the words back into a string\n",
    "    return ' '.join(filtered_words)\n",
    "# Apply the stopword removal to the 'bio_expanded' column\n",
    "ed_combined['full_text_cleaned'] = ed_combined['full_text_expanded'].apply(\n",
    "    lambda x: remove_stopwords(x, reddit_bio_stopwords)\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame with cleaned bios\n",
    "print(ed_combined[['username', 'full_text_cleaned']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d17d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       username                                    comment_cleaned\n",
      "0  SolidLow9296  companies t t care welfare community they re s...\n",
      "1  SolidLow9296  aw man position open canada mandarin language ...\n",
      "2  SolidLow9296  cool yes got pet gate time split house 2 separ...\n",
      "3  SolidLow9296  would say pessimistic definitely know younger ...\n",
      "4  SolidLow9296  interests including information technology han...\n"
     ]
    }
   ],
   "source": [
    "# Apply the stopword removal to the 'bio_expanded' column\n",
    "recover_comments_only['comment_cleaned'] = recover_comments_only['comment_expanded'].apply(\n",
    "    lambda x: remove_stopwords(x, reddit_bio_stopwords)\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame with cleaned bios\n",
    "print(recover_comments_only[['username', 'comment_cleaned']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a4dce",
   "metadata": {},
   "source": [
    "### lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6dc8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               username                                 comment_lemmatized\n",
      "0  Embarrassed-Local-79  thank significant othershout much take time en...\n",
      "1  Embarrassed-Local-79  know deep something wait see wrong something w...\n",
      "2  Embarrassed-Local-79  thank guidance information technology work man...\n",
      "3  Embarrassed-Local-79  wait see definitely low grade marijuana gossip...\n",
      "4  Embarrassed-Local-79                  cool freestyle improvisation home\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a function to lemmatize a sentence\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    doc = nlp(text)\n",
    "    lemmatized = [token.lemma_ for token in doc if token.is_alpha or token.is_digit]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Apply to the expanded bios\n",
    "ed_comments_only['comment_lemmatized'] = ed_comments_only['comment_cleaned'].apply(lemmatize_text)\n",
    "\n",
    "# Display results\n",
    "print(ed_comments_only[['username', 'comment_lemmatized']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619db92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       username                                 comment_lemmatized\n",
      "0  SolidLow9296   company t t care welfare community they re sadly\n",
      "1  SolidLow9296  aw man position open canada mandarin language ...\n",
      "2  SolidLow9296  cool yes get pet gate time split house 2 separ...\n",
      "3  SolidLow9296  would say pessimistic definitely know young ki...\n",
      "4  SolidLow9296  interest include information technology hang h...\n"
     ]
    }
   ],
   "source": [
    "# Apply to the expanded bios\n",
    "recover_comments_only['comment_lemmatized'] = recover_comments_only['comment_cleaned'].apply(lemmatize_text)\n",
    "\n",
    "# Display results\n",
    "print(recover_comments_only[['username', 'comment_lemmatized']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05299d5a",
   "metadata": {},
   "source": [
    "### Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f61b3686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my475/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/anaconda3/envs/my475/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  from scipy.sparse import csr_matrix, issparse\n",
      "/opt/anaconda3/envs/my475/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  from scipy.sparse import csr_matrix, issparse\n",
      "/opt/anaconda3/envs/my475/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  from scipy.sparse import csr_matrix, issparse\n",
      "/opt/anaconda3/envs/my475/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>279937</td>\n",
       "      <td>-1_time_information_technology_miss</td>\n",
       "      <td>[time, information, technology, miss, see, hig...</td>\n",
       "      <td>[agree comment want time attention deficit dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2556</td>\n",
       "      <td>0____</td>\n",
       "      <td>[, , , , , , , , , ]</td>\n",
       "      <td>[, , ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1589</td>\n",
       "      <td>1_thank___</td>\n",
       "      <td>[thank, , , , , , , , , ]</td>\n",
       "      <td>[thank, thank, thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>524</td>\n",
       "      <td>2_yes___</td>\n",
       "      <td>[yes, , , , , , , , , ]</td>\n",
       "      <td>[yes, yes, yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>345</td>\n",
       "      <td>3_thank___</td>\n",
       "      <td>[thank, , , , , , , , , ]</td>\n",
       "      <td>[thank, thank, thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>240</td>\n",
       "      <td>4_thank_much_othershout_significant</td>\n",
       "      <td>[thank, much, othershout, significant, , , , ,...</td>\n",
       "      <td>[thank significant othershout much, thank sign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>198</td>\n",
       "      <td>5_amplitude_midday_modulation_ante</td>\n",
       "      <td>[amplitude, midday, modulation, ante, radio, m...</td>\n",
       "      <td>[ante meridiem midday amplitude modulation rad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>171</td>\n",
       "      <td>6_no___</td>\n",
       "      <td>[no, , , , , , , , , ]</td>\n",
       "      <td>[no, no, no]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>167</td>\n",
       "      <td>7_thank_much_othershout_significant</td>\n",
       "      <td>[thank, much, othershout, significant, , , , ,...</td>\n",
       "      <td>[thank significant othershout much, thank sign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>144</td>\n",
       "      <td>8_update___</td>\n",
       "      <td>[update, , , , , , , , , ]</td>\n",
       "      <td>[update, update, update]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic   Count                                 Name  \\\n",
       "0     -1  279937  -1_time_information_technology_miss   \n",
       "1      0    2556                                0____   \n",
       "2      1    1589                           1_thank___   \n",
       "3      2     524                             2_yes___   \n",
       "4      3     345                           3_thank___   \n",
       "5      4     240  4_thank_much_othershout_significant   \n",
       "6      5     198   5_amplitude_midday_modulation_ante   \n",
       "7      6     171                              6_no___   \n",
       "8      7     167  7_thank_much_othershout_significant   \n",
       "9      8     144                          8_update___   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [time, information, technology, miss, see, hig...   \n",
       "1                               [, , , , , , , , , ]   \n",
       "2                          [thank, , , , , , , , , ]   \n",
       "3                            [yes, , , , , , , , , ]   \n",
       "4                          [thank, , , , , , , , , ]   \n",
       "5  [thank, much, othershout, significant, , , , ,...   \n",
       "6  [amplitude, midday, modulation, ante, radio, m...   \n",
       "7                             [no, , , , , , , , , ]   \n",
       "8  [thank, much, othershout, significant, , , , ,...   \n",
       "9                         [update, , , , , , , , , ]   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [agree comment want time attention deficit dis...  \n",
       "1                                             [, , ]  \n",
       "2                              [thank, thank, thank]  \n",
       "3                                    [yes, yes, yes]  \n",
       "4                              [thank, thank, thank]  \n",
       "5  [thank significant othershout much, thank sign...  \n",
       "6  [ante meridiem midday amplitude modulation rad...  \n",
       "7                                       [no, no, no]  \n",
       "8  [thank significant othershout much, thank sign...  \n",
       "9                           [update, update, update]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Assuming you have the cleaned bios\n",
    "docs = ed_comments_only['comment_lemmatized'].dropna().tolist()\n",
    "\n",
    "# Create BERTopic model\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# View top 10 topics\n",
    "topic_model.get_topic_info().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6b3102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['time', 'want', 'git', 'try', 'know', 'need', 'people', 'good', 'eat', 'note']\n",
      "Topic 1:  ['thank', 'heart', 'happy', 'red', 'appreciate', 'll', 'helpful', 'really', 'hand', 'smile']\n",
      "Topic 2:  ['information', 'technology', 'wait', 'make', 'think', 'really', 'doesn', 'git', 'work', 'help']\n",
      "Topic 3:  ['quote', 'um', 'introduction', 'feel', 'look', 'sound', 'really', 'people', 'good', 'feeling']\n",
      "Topic 4:  ['happy', 'high', 'explosive', 'end', 'drunk', 'wait', 'say', 'doesn', 'didn', 'guy']\n",
      "Topic 5:  ['significant', 'othershout', 'good', 'sorry', 'look', 'awesome', 'feel', 'really', 'heart', 'hard']\n",
      "Topic 6:  ['face', 'loudly', 'smile', 'tear', 'heart', 'joy', 'eye', 'sweat', 'red', 'grin']\n",
      "Topic 7:  ['meridiem', 'radio', 'modulation', 'midday', 'amplitude', 'ante', 'signal', 'sorry', 'miss', 'feel']\n",
      "Topic 8:  ['love', 'goodbye', 'heart', 'shape', 'red', 'good', 'day', 'really', 'god', 'great']\n",
      "Topic 9:  ['miss', 'wait', 'yes', 'think', 'laugh', 'loud', 'drunk', 'high', 'year', 'friend']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "nmf = NMF(n_components=10, random_state=42)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "# Display top words per topic\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i, topic in enumerate(H):\n",
    "    print(f\"Topic {i}: \", [terms[j] for j in topic.argsort()[-10:][::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64203db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Topic 0 (Self-Direction): ['year', 'look', 'skin', 'woman', 'light', 'doctor', 'sexy', 'tone', 'period', 'hot']\n",
      "🔹 Topic 1 (Stimulation): ['goodbye', 'one', 'love', 'sarcastic', 'often', 'yes', 'clinical', 'excellence', 'national', 'institute']\n",
      "🔹 Topic 2 (Achievement): ['gang', 'com', 'goal', 'program', 'www', 'division', 'computer', 'member', 'close', 'reddit']\n",
      "🔹 Topic 3 (Power): ['not', 'note', 'author', 'think', 'people', 'even', 'eat', 'body', 'need', 'control']\n",
      "🔹 Topic 4 (Security): ['they', 'their', 'many', 'child', 'support', 'protect', 'type', 'place', 'aren', 'mean']\n",
      "🔹 Topic 5 (Conformity): ['high', 'drunk', 'end', 'explosive', 'happy', 'say', 'he', 'she', 'would', 'her']\n",
      "🔹 Topic 6 (Tradition): ['webp', 'preview', 'width', 'redd', 'auto', 'format', 'pjpg', 'jpeg', 'https', 'en']\n",
      "🔹 Topic 7 (Benevolence): ['time', 'information', 'technology', 'miss', 'quote', 'um', 'introduction', 'significant', 'othershout', 'see']\n",
      "🔹 Topic 8 (Universalism): ['united', 'states', 'cocaine', 'music', 'powder', 'black', 'nature', 'game', 'line', 'play']\n",
      "⚠️ Missing anchor words: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from corextopic.corextopic import Corex\n",
    "\n",
    "# 1. Prepare your lemmatized posts list\n",
    "docs = ed_comments_only['comment_lemmatized'].dropna().tolist()\n",
    "\n",
    "# 2. Vectorize to document-word matrix\n",
    "vectorizer = CountVectorizer(min_df=1, binary=True)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 3. Define anchor seed word sets for CorEx\n",
    "anchors = [\n",
    "    # Schwartz’s Basic Human Values\n",
    "    [\"independence\", \"autonomy\", \"explore\"],                       # Self-Direction\n",
    "    [\"excite\", \"novel\", \"challenge\"],                              # Stimulation\n",
    "    [\"success\", \"competence\", \"goal\", \"ambitious\"],                # Achievement\n",
    "    [\"control\", \"status\", \"prestige\", \"dominance\"],                # Power\n",
    "    [\"safety\", \"harmony\", \"stable\", \"protect\"],                    # Security\n",
    "    [\"rule\", \"obedience\", \"norm\", \"respect\"],                      # Conformity\n",
    "    [\"custom\", \"heritage\", \"ritual\", \"sacred\"],                    # Tradition\n",
    "    [\"help\", \"caring\", \"loyalty\", \"kindness\"],                     # Benevolence\n",
    "    [\"tolerance\", \"equality\", \"nature\", \"justice\"],                # Universalism\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"Self-Direction\", \"Stimulation\", \"Achievement\", \"Power\", \"Security\",\n",
    "    \"Conformity\", \"Tradition\", \"Benevolence\", \"Universalism\"\n",
    "]\n",
    "\n",
    "# 4. Fit Anchored CorEx\n",
    "model = Corex(n_hidden=len(anchors), seed=42)\n",
    "model.fit(X, words=feature_names, anchors=anchors, anchor_strength=2)\n",
    "\n",
    "# 5. Print top words per topic\n",
    "for idx, topic in enumerate(model.get_topics()):\n",
    "    words = [w for w, mi, s in topic]\n",
    "    print(f\"🔹 Topic {idx} ({labels[idx]}): {words}\")\n",
    "\n",
    "# 6. Check if any anchor terms are missing from vocabulary\n",
    "missing = [word for topic in anchors for word in topic if word not in feature_names]\n",
    "print(\"⚠️ Missing anchor words:\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccf02cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my475/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  from scipy.sparse import csr_matrix, issparse\n",
      "/opt/anaconda3/envs/my475/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  from scipy.sparse import csr_matrix, issparse\n",
      "/opt/anaconda3/envs/my475/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  from scipy.sparse import csr_matrix, issparse\n",
      "/opt/anaconda3/envs/my475/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>298182</td>\n",
       "      <td>-1_time_miss_technology_information</td>\n",
       "      <td>[time, miss, technology, information, high, th...</td>\n",
       "      <td>[author s note excellent female doctor right h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3245</td>\n",
       "      <td>0____</td>\n",
       "      <td>[, , , , , , , , , ]</td>\n",
       "      <td>[, , ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1372</td>\n",
       "      <td>1_thank___</td>\n",
       "      <td>[thank, , , , , , , , , ]</td>\n",
       "      <td>[thank, thank, thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>634</td>\n",
       "      <td>2_yes___</td>\n",
       "      <td>[yes, , , , , , , , , ]</td>\n",
       "      <td>[yes, yes, yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>269</td>\n",
       "      <td>3_goodbye_one_love_favorite</td>\n",
       "      <td>[goodbye, one, love, favorite, second, oh, cir...</td>\n",
       "      <td>[one love goodbye, one love goodbye, one love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>4_no___</td>\n",
       "      <td>[no, , , , , , , , , ]</td>\n",
       "      <td>[no, no, no]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>252</td>\n",
       "      <td>5_technology_information_acotar_tangible</td>\n",
       "      <td>[technology, information, acotar, tangible, bi...</td>\n",
       "      <td>[think information technology, think informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>236</td>\n",
       "      <td>6_thank___</td>\n",
       "      <td>[thank, , , , , , , , , ]</td>\n",
       "      <td>[thank, thank, thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>232</td>\n",
       "      <td>7_thank_happy__</td>\n",
       "      <td>[thank, happy, , , , , , , , ]</td>\n",
       "      <td>[thank happy, thank happy, thank happy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>195</td>\n",
       "      <td>8_happy___</td>\n",
       "      <td>[happy, , , , , , , , , ]</td>\n",
       "      <td>[happy, happy, happy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic   Count                                      Name  \\\n",
       "0     -1  298182       -1_time_miss_technology_information   \n",
       "1      0    3245                                     0____   \n",
       "2      1    1372                                1_thank___   \n",
       "3      2     634                                  2_yes___   \n",
       "4      3     269               3_goodbye_one_love_favorite   \n",
       "5      4     256                                   4_no___   \n",
       "6      5     252  5_technology_information_acotar_tangible   \n",
       "7      6     236                                6_thank___   \n",
       "8      7     232                           7_thank_happy__   \n",
       "9      8     195                                8_happy___   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [time, miss, technology, information, high, th...   \n",
       "1                               [, , , , , , , , , ]   \n",
       "2                          [thank, , , , , , , , , ]   \n",
       "3                            [yes, , , , , , , , , ]   \n",
       "4  [goodbye, one, love, favorite, second, oh, cir...   \n",
       "5                             [no, , , , , , , , , ]   \n",
       "6  [technology, information, acotar, tangible, bi...   \n",
       "7                          [thank, , , , , , , , , ]   \n",
       "8                     [thank, happy, , , , , , , , ]   \n",
       "9                          [happy, , , , , , , , , ]   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [author s note excellent female doctor right h...  \n",
       "1                                             [, , ]  \n",
       "2                              [thank, thank, thank]  \n",
       "3                                    [yes, yes, yes]  \n",
       "4  [one love goodbye, one love goodbye, one love ...  \n",
       "5                                       [no, no, no]  \n",
       "6  [think information technology, think informati...  \n",
       "7                              [thank, thank, thank]  \n",
       "8            [thank happy, thank happy, thank happy]  \n",
       "9                              [happy, happy, happy]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Assuming you have the cleaned bios\n",
    "recover_docs = recover_comments_only['comment_lemmatized'].dropna().tolist()\n",
    "\n",
    "# Create BERTopic model\n",
    "recover_topic_model = BERTopic()\n",
    "topics, probs = recover_topic_model.fit_transform(recover_docs)\n",
    "\n",
    "# View top 10 topics\n",
    "\n",
    "recover_topic_model.get_topic_info().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bacf780b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['time', 'want', 'git', 'try', 'know', 'people', 'need', 'say', 'use', 'make']\n",
      "Topic 1:  ['thank', 'happy', 'heart', 'll', 'appreciate', 'smile', 'advice', 'red', 'hand', 'try']\n",
      "Topic 2:  ['happy', 'high', 'end', 'explosive', 'drunk', 'say', 'wait', 'doesn', 'know', 'guy']\n",
      "Topic 3:  ['um', 'quote', 'introduction', 'look', 'feel', 'sound', 'people', 'really', 'good', 'cool']\n",
      "Topic 4:  ['information', 'technology', 'wait', 'make', 'really', 'think', 'work', 'doesn', 'look', 'know']\n",
      "Topic 5:  ['face', 'loudly', 'smile', 'tear', 'joy', 'eye', 'heart', 'sweat', 'grin', 'hand']\n",
      "Topic 6:  ['significant', 'othershout', 'good', 'look', 'heart', 'sorry', 'awesome', 'feel', 'cool', 'really']\n",
      "Topic 7:  ['radio', 'meridiem', 'signal', 'modulation', 'ante', 'amplitude', 'midday', 'subreddit', 'message', 'moderator']\n",
      "Topic 8:  ['love', 'goodbye', 'heart', 'shape', 'red', 'day', 'god', 'good', 'oh', 'hope']\n",
      "Topic 9:  ['miss', 'wait', 'think', 'drunk', 'yes', 'high', 'laugh', 'loud', 'good', 'really']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "X = vectorizer.fit_transform(recover_docs)\n",
    "\n",
    "nmf = NMF(n_components=10, random_state=42)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "# Display top words per topic\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i, topic in enumerate(H):\n",
    "    print(f\"Topic {i}: \", [terms[j] for j in topic.argsort()[-10:][::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce9c32cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Topic 0 (Self-Direction): ['year', 'look', 'skin', 'woman', 'light', 'doctor', 'sexy', 'tone', 'period', 'hot']\n",
      "🔹 Topic 1 (Stimulation): ['goodbye', 'one', 'love', 'sarcastic', 'often', 'yes', 'clinical', 'excellence', 'national', 'institute']\n",
      "🔹 Topic 2 (Achievement): ['gang', 'com', 'goal', 'program', 'www', 'division', 'computer', 'member', 'close', 'reddit']\n",
      "🔹 Topic 3 (Power): ['not', 'note', 'author', 'think', 'people', 'even', 'eat', 'body', 'need', 'control']\n",
      "🔹 Topic 4 (Security): ['they', 'their', 'many', 'child', 'support', 'protect', 'type', 'place', 'aren', 'mean']\n",
      "🔹 Topic 5 (Conformity): ['high', 'drunk', 'end', 'explosive', 'happy', 'say', 'he', 'she', 'would', 'her']\n",
      "🔹 Topic 6 (Tradition): ['webp', 'preview', 'width', 'redd', 'auto', 'format', 'pjpg', 'jpeg', 'https', 'en']\n",
      "🔹 Topic 7 (Benevolence): ['time', 'information', 'technology', 'miss', 'quote', 'um', 'introduction', 'significant', 'othershout', 'see']\n",
      "🔹 Topic 8 (Universalism): ['united', 'states', 'cocaine', 'music', 'powder', 'black', 'nature', 'game', 'line', 'play']\n",
      "⚠️ Missing anchor words: []\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare your lemmatized posts list\n",
    "recover_docs = ed_comments_only['comment_lemmatized'].dropna().tolist()\n",
    "\n",
    "# 2. Vectorize to document-word matrix\n",
    "vectorizer = CountVectorizer(min_df=1, binary=True)\n",
    "X = vectorizer.fit_transform(recover_docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 3. Define anchor seed word sets for CorEx\n",
    "anchors = [\n",
    "    # Schwartz’s Basic Human Values\n",
    "    [\"independence\", \"autonomy\", \"explore\"],                       # Self-Direction\n",
    "    [\"excite\", \"novel\", \"challenge\"],                              # Stimulation\n",
    "    [\"success\", \"competence\", \"goal\", \"ambitious\"],                # Achievement\n",
    "    [\"control\", \"status\", \"prestige\", \"dominance\"],                # Power\n",
    "    [\"safety\", \"harmony\", \"stable\", \"protect\"],                    # Security\n",
    "    [\"rule\", \"obedience\", \"norm\", \"respect\"],                      # Conformity\n",
    "    [\"custom\", \"heritage\", \"ritual\", \"sacred\"],                    # Tradition\n",
    "    [\"help\", \"caring\", \"loyalty\", \"kindness\"],                     # Benevolence\n",
    "    [\"tolerance\", \"equality\", \"nature\", \"justice\"],                # Universalism\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"Self-Direction\", \"Stimulation\", \"Achievement\", \"Power\", \"Security\",\n",
    "    \"Conformity\", \"Tradition\", \"Benevolence\", \"Universalism\"\n",
    "]\n",
    "\n",
    "# 4. Fit Anchored CorEx\n",
    "model = Corex(n_hidden=len(anchors), seed=42)\n",
    "model.fit(X, words=feature_names, anchors=anchors, anchor_strength=2)\n",
    "\n",
    "# 5. Print top words per topic\n",
    "for idx, topic in enumerate(model.get_topics()):\n",
    "    words = [w for w, mi, s in topic]\n",
    "    print(f\"🔹 Topic {idx} ({labels[idx]}): {words}\")\n",
    "\n",
    "# 6. Check if any anchor terms are missing from vocabulary\n",
    "missing = [word for topic in anchors for word in topic if word not in feature_names]\n",
    "print(\"⚠️ Missing anchor words:\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9ed7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "schwartz_labels = [\n",
    "    \"valuing independence, autonomy, and exploration\",\n",
    "    \"seeking excitement, novelty, and challenge\",\n",
    "    \"striving for success, competence, and achievement\",\n",
    "    \"valuing control, power, and social status\",\n",
    "    \"prioritizing safety, harmony, and stability\",\n",
    "    \"following rules, norms, and showing respect for authority\",\n",
    "    \"preserving tradition, heritage, and sacred customs\",\n",
    "    \"caring for others and showing kindness and loyalty\",\n",
    "    \"promoting tolerance, equality, and social justice\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "336e4d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Classifying text: 100%|██████████| 2854/2854 [43:24:09<00:00, 54.75s/it]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Topic Scores:\n",
      "caring for others and showing kindness and loyalty           0.823782\n",
      "seeking excitement, novelty, and challenge                   0.816432\n",
      "valuing control, power, and social status                    0.794298\n",
      "following rules, norms, and showing respect for authority    0.772964\n",
      "prioritizing safety, harmony, and stability                  0.744950\n",
      "valuing independence, autonomy, and exploration              0.738398\n",
      "striving for success, competence, and achievement            0.639575\n",
      "preserving tradition, heritage, and sacred customs           0.633247\n",
      "promoting tolerance, equality, and social justice            0.593360\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "ed_com = ed_combined['full_text_expanded'].dropna()\n",
    "ed_com = ed_com[ed_com.str.strip() != ''].tolist()\n",
    "\n",
    "# 4. Apply zero-shot classification\n",
    "recover_classified_results = []\n",
    "for text in tqdm(ed_com, desc=\"Classifying text\"):\n",
    "    try:\n",
    "        result = classifier(text, candidate_labels=schwartz_labels, multi_label=True)\n",
    "        scores = dict(zip(result['labels'], result['scores']))\n",
    "        scores['text'] = text\n",
    "        recover_classified_results.append(scores)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped due to error: {e}\")\n",
    "\n",
    "# 5. Convert results to DataFrame\n",
    "recover_classified_df = pd.DataFrame(recover_classified_results)\n",
    "\n",
    "# 6. View average relevance scores per topic\n",
    "recover_average_scores = recover_classified_df.drop(columns=[\"text\"]).mean().sort_values(ascending=False)\n",
    "print(\"Average Topic Scores:\")\n",
    "print(recover_average_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93b5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying comments: 100%|██████████| 2886/2886 [29:01:25<00:00, 36.20s/it]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Topic Scores:\n",
      "caring for others and showing kindness and loyalty           0.839820\n",
      "seeking excitement, novelty, and challenge                   0.821678\n",
      "valuing control, power, and social status                    0.808837\n",
      "following rules, norms, and showing respect for authority    0.797486\n",
      "prioritizing safety, harmony, and stability                  0.769529\n",
      "valuing independence, autonomy, and exploration              0.748081\n",
      "striving for success, competence, and achievement            0.660607\n",
      "preserving tradition, heritage, and sacred customs           0.653212\n",
      "promoting tolerance, equality, and social justice            0.607647\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "recover_com  = recover_combined['full_text_expanded'].dropna()\n",
    "recover_com  = recover_com [recover_com.str.strip() != ''].tolist()\n",
    "\n",
    "\n",
    "# 4. Apply zero-shot classification\n",
    "classified_results = []\n",
    "for text in tqdm(recover_com, desc=\"Classifying comments\"):  # You can remove [:100] to do full dataset\n",
    "    try:\n",
    "        result = classifier(text, candidate_labels=schwartz_labels, multi_label=True)\n",
    "        scores = dict(zip(result['labels'], result['scores']))\n",
    "        scores['texts'] = text\n",
    "        classified_results.append(scores)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped due to error: {e}\")\n",
    "\n",
    "# 5. Convert results to DataFrame\n",
    "classified_df = pd.DataFrame(classified_results)\n",
    "\n",
    "# 6. View average relevance scores per topic\n",
    "average_scores = classified_df.drop(columns=[\"texts\"]).mean().sort_values(ascending=False)\n",
    "print(\"Average Topic Scores:\")\n",
    "print(average_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b7ce77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schwartz Basic Human Values with definitions\n",
    "schwartz_def_labels = [\n",
    "    \"Self-Direction: Independent thought and action—choosing, creating, exploring\",\n",
    "    \"Stimulation: Excitement, novelty, and challenge in life\",\n",
    "    \"Hedonism: Pleasure and sensuous gratification for oneself\",\n",
    "    \"Achievement: Personal success through demonstrating competence according to social standards\",\n",
    "    \"Power: Social status and prestige, control or dominance over people and resources\",\n",
    "    \"Security: Safety, harmony, and stability of society, of relationships, and of self\",\n",
    "    \"Conformity: Restraint of actions, inclinations, and impulses likely to upset or harm others and violate social expectations or norms\",\n",
    "    \"Tradition: Respect, commitment, and acceptance of the customs and ideas that traditional culture or religion provide\",\n",
    "    \"Benevolence: Preserving and enhancing the welfare of those with whom one is in frequent personal contact\",\n",
    "    \"Universalism: Understanding, appreciation, tolerance, and protection for the welfare of all people and for nature\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76282c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2854\n",
      "2886\n"
     ]
    }
   ],
   "source": [
    "print(len(ed_com))\n",
    "print(len(recover_com))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfef6179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Classifying text: 100%|██████████| 2854/2854 [9:09:25<00:00, 11.55s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Topic Scores:\n",
      "Benevolence: Preserving and enhancing the welfare of those with whom one is in frequent personal contact                                0.084938\n",
      "Self-Direction: Independent thought and action—choosing, creating, exploring                                                            0.059841\n",
      "Stimulation: Excitement, novelty, and challenge in life                                                                                 0.052560\n",
      "Hedonism: Pleasure and sensuous gratification for oneself                                                                               0.044698\n",
      "Conformity: Restraint of actions, inclinations, and impulses likely to upset or harm others and violate social expectations or norms    0.033507\n",
      "Security: Safety, harmony, and stability of society, of relationships, and of self                                                      0.026785\n",
      "Universalism: Understanding, appreciation, tolerance, and protection for the welfare of all people and for nature                       0.012727\n",
      "Tradition: Respect, commitment, and acceptance of the customs and ideas that traditional culture or religion provide                    0.012428\n",
      "Power: Social status and prestige, control or dominance over people and resources                                                       0.011970\n",
      "Achievement: Personal success through demonstrating competence according to social standards                                            0.011582\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\")\n",
    "\n",
    "ed_com = ed_combined['full_text_expanded'].dropna()\n",
    "ed_com = ed_com[ed_com.str.strip() != ''].tolist()\n",
    "\n",
    "# 4. Apply zero-shot classification\n",
    "ed_classified_results = []\n",
    "for text in tqdm(ed_com, desc=\"Classifying text\"):\n",
    "    try:\n",
    "        result = classifier(text, candidate_labels=schwartz_def_labels, multi_label=True)\n",
    "        scores = dict(zip(result['labels'], result['scores']))\n",
    "        scores['text'] = text\n",
    "        ed_classified_results.append(scores)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped due to error: {e}\")\n",
    "\n",
    "# 5. Convert results to DataFrame\n",
    "ed_classified_df = pd.DataFrame(ed_classified_results)\n",
    "\n",
    "# 6. View average relevance scores per topic\n",
    "ed_average_scores = ed_classified_df.drop(columns=[\"text\"]).mean().sort_values(ascending=False)\n",
    "print(\"Average Topic Scores:\")\n",
    "print(ed_average_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71ceabb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying comments: 100%|██████████| 2886/2886 [5:49:14<00:00,  7.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Topic Scores:\n",
      "Benevolence: Preserving and enhancing the welfare of those with whom one is in frequent personal contact                                0.086741\n",
      "Self-Direction: Independent thought and action—choosing, creating, exploring                                                            0.052299\n",
      "Stimulation: Excitement, novelty, and challenge in life                                                                                 0.048783\n",
      "Hedonism: Pleasure and sensuous gratification for oneself                                                                               0.037239\n",
      "Conformity: Restraint of actions, inclinations, and impulses likely to upset or harm others and violate social expectations or norms    0.033360\n",
      "Security: Safety, harmony, and stability of society, of relationships, and of self                                                      0.029164\n",
      "Universalism: Understanding, appreciation, tolerance, and protection for the welfare of all people and for nature                       0.015928\n",
      "Tradition: Respect, commitment, and acceptance of the customs and ideas that traditional culture or religion provide                    0.013695\n",
      "Achievement: Personal success through demonstrating competence according to social standards                                            0.013221\n",
      "Power: Social status and prestige, control or dominance over people and resources                                                       0.011678\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recover_com  = recover_combined['full_text_expanded'].dropna()\n",
    "recover_com  = recover_com [recover_com.str.strip() != ''].tolist()\n",
    "\n",
    "\n",
    "# 4. Apply zero-shot classification\n",
    "recover_classified_results = []\n",
    "for text in tqdm(recover_com, desc=\"Classifying comments\"):  # You can remove [:100] to do full dataset\n",
    "    try:\n",
    "        result = classifier(text, candidate_labels=schwartz_def_labels, multi_label=True)\n",
    "        scores = dict(zip(result['labels'], result['scores']))\n",
    "        scores['texts'] = text\n",
    "        recover_classified_results.append(scores)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped due to error: {e}\")\n",
    "\n",
    "# 5. Convert results to DataFrame\n",
    "recover_classified_df = pd.DataFrame(recover_classified_results)\n",
    "\n",
    "# 6. View average relevance scores per topic\n",
    "recover_average_scores = recover_classified_df.drop(columns=[\"texts\"]).mean().sort_values(ascending=False)\n",
    "print(\"Average Topic Scores:\")\n",
    "print(recover_average_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5cc2696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Stimulation: Excitement, novelty, and challenge in life  \\\n",
      "0                                           0.227126         \n",
      "1                                           0.001697         \n",
      "2                                           0.000922         \n",
      "3                                           0.004486         \n",
      "4                                           0.000836         \n",
      "\n",
      "   Self-Direction: Independent thought and action—choosing, creating, exploring  \\\n",
      "0                                           0.164584                              \n",
      "1                                           0.003337                              \n",
      "2                                           0.015785                              \n",
      "3                                           0.087798                              \n",
      "4                                           0.014842                              \n",
      "\n",
      "   Tradition: Respect, commitment, and acceptance of the customs and ideas that traditional culture or religion provide  \\\n",
      "0                                           0.129899                                                                      \n",
      "1                                           0.005708                                                                      \n",
      "2                                           0.010653                                                                      \n",
      "3                                           0.024008                                                                      \n",
      "4                                           0.004417                                                                      \n",
      "\n",
      "   Conformity: Restraint of actions, inclinations, and impulses likely to upset or harm others and violate social expectations or norms  \\\n",
      "0                                           0.040693                                                                                      \n",
      "1                                           0.020081                                                                                      \n",
      "2                                           0.036494                                                                                      \n",
      "3                                           0.020004                                                                                      \n",
      "4                                           0.009541                                                                                      \n",
      "\n",
      "   Power: Social status and prestige, control or dominance over people and resources  \\\n",
      "0                                           0.032131                                   \n",
      "1                                           0.005664                                   \n",
      "2                                           0.009143                                   \n",
      "3                                           0.002946                                   \n",
      "4                                           0.003173                                   \n",
      "\n",
      "   Hedonism: Pleasure and sensuous gratification for oneself  \\\n",
      "0                                           0.025131           \n",
      "1                                           0.015754           \n",
      "2                                           0.001772           \n",
      "3                                           0.114220           \n",
      "4                                           0.002103           \n",
      "\n",
      "   Universalism: Understanding, appreciation, tolerance, and protection for the welfare of all people and for nature  \\\n",
      "0                                           0.025035                                                                   \n",
      "1                                           0.003984                                                                   \n",
      "2                                           0.014448                                                                   \n",
      "3                                           0.010673                                                                   \n",
      "4                                           0.005675                                                                   \n",
      "\n",
      "   Security: Safety, harmony, and stability of society, of relationships, and of self  \\\n",
      "0                                           0.012797                                    \n",
      "1                                           0.077100                                    \n",
      "2                                           0.031649                                    \n",
      "3                                           0.008062                                    \n",
      "4                                           0.013299                                    \n",
      "\n",
      "   Benevolence: Preserving and enhancing the welfare of those with whom one is in frequent personal contact  \\\n",
      "0                                           0.009038                                                          \n",
      "1                                           0.333937                                                          \n",
      "2                                           0.047250                                                          \n",
      "3                                           0.038865                                                          \n",
      "4                                           0.011801                                                          \n",
      "\n",
      "   Achievement: Personal success through demonstrating competence according to social standards  \\\n",
      "0                                           0.006376                                              \n",
      "1                                           0.005598                                              \n",
      "2                                           0.009340                                              \n",
      "3                                           0.004256                                              \n",
      "4                                           0.000838                                              \n",
      "\n",
      "                                               texts  \n",
      "0  great really good boss can you show united sta...  \n",
      "1  i really really want time out binge but i m tw...  \n",
      "2  i ve only heard good things about the s1 drunk...  \n",
      "3  removed removed im about six months into recov...  \n",
      "4  i ante meridiem before midday amplitude modula...  \n"
     ]
    }
   ],
   "source": [
    "print(recover_classified_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afaa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Value   ED Mean  Recovery Mean  \\\n",
      "9  Universalism: Understanding, appreciation, tol...  0.012727       0.015928   \n",
      "5  Hedonism: Pleasure and sensuous gratification ...  0.044698       0.037239   \n",
      "1  Self-Direction: Independent thought and action...  0.059841       0.052299   \n",
      "2  Achievement: Personal success through demonstr...  0.011582       0.013221   \n",
      "6  Security: Safety, harmony, and stability of so...  0.026785       0.029164   \n",
      "8  Tradition: Respect, commitment, and acceptance...  0.012428       0.013695   \n",
      "0  Stimulation: Excitement, novelty, and challeng...  0.052560       0.048783   \n",
      "4  Benevolence: Preserving and enhancing the welf...  0.084938       0.086741   \n",
      "7  Power: Social status and prestige, control or ...  0.011970       0.011678   \n",
      "3  Conformity: Restraint of actions, inclinations...  0.033507       0.033360   \n",
      "\n",
      "     t-test  t-test p  Mann-Whitney p  Significant (p<.05)  \n",
      "9 -2.442549  0.014618        0.329683                 True  \n",
      "5  2.130201  0.033198        0.996292                 True  \n",
      "1  1.950691  0.051143        0.930504                False  \n",
      "2 -1.528779  0.126377        0.189786                False  \n",
      "6 -1.204055  0.228619        0.215652                False  \n",
      "8 -1.100465  0.271176        0.000064                False  \n",
      "0  0.952976  0.340642        0.652987                False  \n",
      "4 -0.505111  0.613500        0.210003                False  \n",
      "7  0.265824  0.790384        0.468482                False  \n",
      "3  0.100343  0.920076        0.307139                False  \n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "import pandas as pd\n",
    "\n",
    "schwartz_labels = list(ed_classified_df.columns)\n",
    "schwartz_labels.remove(\"text\")  # 或 \"texts\"，根据你 DataFrame 的实际情况调整\n",
    "\n",
    "results = []\n",
    "\n",
    "for label in schwartz_labels:\n",
    "    ed_scores = ed_classified_df[label].dropna()\n",
    "    rec_scores = recover_classified_df[label].dropna()\n",
    "\n",
    "    t_stat, t_p = ttest_ind(ed_scores, rec_scores, equal_var=False)\n",
    "\n",
    "\n",
    "    u_stat, u_p = mannwhitneyu(ed_scores, rec_scores, alternative='less')\n",
    "\n",
    "    results.append({\n",
    "        \"Value\": label,\n",
    "        \"ED Mean\": ed_scores.mean(),\n",
    "        \"Recovery Mean\": rec_scores.mean(),\n",
    "        \"t-test\":t_stat,\n",
    "        \"t-test p\": t_p,\n",
    "        \"Mann-Whitney p\": u_p\n",
    "    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df[\"Significant (p<.05)\"] = results_df[\"t-test p\"] < 0.05\n",
    "print(results_df.sort_values(\"t-test p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b516e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(ed_classified_df).to_csv(\"ed_zero_shot_result.csv\", index=False)\n",
    "pd.DataFrame(recover_classified_df).to_csv(\"recovery_zero_shot_result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my475",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
